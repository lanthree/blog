<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>翻译 on Engineers Cool</title>
    <link>https://engineers.cool/categories/%E7%BF%BB%E8%AF%91/</link>
    <description>Recent content in 翻译 on Engineers Cool</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 02 Feb 2022 09:51:01 +0800</lastBuildDate><atom:link href="https://engineers.cool/categories/%E7%BF%BB%E8%AF%91/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Raft</title>
      <link>https://engineers.cool/posts/distribute-system/papers-read/raft/</link>
      <pubDate>Wed, 02 Feb 2022 09:51:01 +0800</pubDate>
      
      <guid>https://engineers.cool/posts/distribute-system/papers-read/raft/</guid>
      <description>In Search of an Understandable Consensus Algorithm
 摘要  原论文在这里
 Raft是一种用于管理副本日志的共识算法（Consensus Algorithm）。它提供一个等同于(multi-)Paxos的结果，效率跟Paxos一样，但是组织结构不同于Paxos；这令Raft比Paxos更利于理解，对于建设实际系统提供了更好的基础。为了增加可理解性，Raft分离了共识的关键元素，例如，leader选举、日志副本和安全(safety)，他主张更强的一致性，来减少必须要考虑的状态的数量。一项用户研究的结果表明，对于学生来说，Raft比Paxos更容易学习。Raft还包括一个改变集群中成员关系的新机制，它使用重叠的多数来保证安全。
1. 介绍 共识算法允许一批机器像 能够经受其部分成员故障的一致的群体 一样工作。因此，在建设可靠的大型软件系统中，共识算法承担着关键角色。在过去十年中，Paxos算法统治着共识算法的讨论：大部分共识的实现都是基于Paxos或受其影响，Paxos也变成教导学生共识算法的重要工具。
不幸的是，尽管人们极力尝试让它平易近人，Paxos还是非常难懂。此外，为了支持实际系统，它的架构需要复杂的变化。结果，系统的构建者和学生都在与Paxos作斗争。
在亲自跟Paxos斗争之后，我们开始寻找一个新的 能为系统构建和教育提供更好基础 共识算法。我们的方法不同寻常，因为我们的首要目标是可理解性：我们是否可以为实际系统定义一个共识算法，并用一种明显比Paxos容易学习的方式描述它？另外，我们希望算法能够促进直觉的发展，这对于系统构建者来说是必不可少的。重要的不仅是算法是否有效，还在于它能明显地被理解为什么有效。
这项工作的成果是一个称为Raft的共识算法。在设计Raft时，我们应用特殊的技术来提高可理解性，包括分解（Raft分解leader选举、日志副本和安全）和减少状态空间（相对于Paxos，Raft减少了非确定性程度和服务器之间保持一致性的方法）。一项涉及2所大学43名学生的用户研究表明，Raft明显比Paxos容易理解：在学习完两个算法后，其中33名学习对Raft问题的回答比Paxos问题要好。
Raft在许多方面与现有的共识算法相似（最明显的是，Oki和Liskov的Viewstamped Replication），但是特有几个新颖的特性：
 强leader（strong leader）：相对于其他共识算法，Raft使用一种更强形式的leader模式。例如，日志条目（log entries）只能从leader向其他server流动。这减缓了备份日志的管理，也让Raft更容易理解。 leader选举（leader election）：Raft使用随机的timer来选举leader。这仅仅在任何共识算法都需要的heartbeat上增加一点点机制，但是简单快速地解决了冲突问题。 成员关系变化（membership changes）：Raft用于更改集群中的服务器集的机制使用了一种新的联合共识（joint consensus）方法，在这种方法中，两种不同配置的大部分服务 在转换期间 重叠（overlap）。这允许集群在配置更改期间继续正常运行。  我们相信，无论出于教育目的 还是最为实现基础，Raft都比Paxos更优秀。Raft比其他算法都简单，可理解性更强；描述完整，足以满足实际系统的需要；它有几个开源实现，被多家公司使用；其安全特性已得到正式规定和证明； 其效率可与其他算法相媲美。
论文的其余部分介绍了副本状态机（replicated state machine）问题（第2节），讨论Paxos的优缺点（第 3节），描述我们实现可理解性的一般方法（第4节），介绍Raft共识算法（第5-8节），评估Raft（第9节），并讨论相关工作（第10节）。
2. 副本状态机 共识算法通常出现在副本状态机（replicated state machine）的上下文中。在这个方法中，一组server的状态机计算相同状态的相同副本，能在其中一些server故障时继续服务。副本状态机用于在分布式系统中解决许多容错问题。举个例子，拥有单集群master的大型系统 例如 GFS、HDFS、RAMCloud，通常用一个分离的副本状态机来管理leader选举和存储配置信息，达到leader crash还能继续服务的目的。副本状态机的例子包括Chubby、ZooKeeper。
如图1所示，副本状态机通常用副本日志实现。每个server存储包含一系列命令的日志，他们的状态机按日志顺序执行。每个日志包含同样顺序的同样命令，所以每个状态机按同样的命令处理同样的命令。因此，这些状态机是确定的，每个计算同样的状态，输出同样的序列。
保持副本日志的一致性就是共识算法的工作。server上的共识模块从多个client接收不同命令，并把命令添加到它的日志中。它与其他server上的共识模块交流，来确保，即使某些server故障，每个日志最终包含相同顺序的同样命令。一旦命令被合适的备份，每个server的状态机按日志顺序处理这些命令，输出被返回给那些client。最终结果，server看上去形成一个单一的、高可用的状态机。
实际系统的共识算法通常具有以下特性：
 算法确保在所有非拜占庭（non-Byzantine）条件下的安全性（safety，从不返回不正确的结果）,包括网络延迟、分区（partitions）、丢包、重入和重排序（reordering）等。  拜占庭问题。简单解释是：有间谍（叛徒），跟不同/相同对象返回的决策结果不一致。   只要任意大多数server可执行且能其他server/client通信，算法就是功能完好的（可用的）。因此，通常5台server的集群，可以容忍任意2台server故障。server假设的故障是停止工作；他们也许稍后会从稳定存储中恢复状态并重新加入集群。 算法不能依赖timing来确保日志的一致性：最坏情况下，时钟错误和极端的消息延迟能导致可用性问题。 通常情况下，只要集群的大多数成员响应了一轮RPC，命令就可以完成；少数速度较慢的服务器需要不影响整个系统的性能。  3. Paxos的问题是什么？ 在过去十年中，Leslit Lamport的Paxos协议几乎已经成为共识的同义词：它是在课堂中最常教授的协议，大多数共识的实现都将其作为起点。Paxos首先定义了一种 能够就单个决定达成一致 的协议，例如单个日志条目备份。我们称这个子集为single-decree Paxos。Paxos然后组合该协议的多个实例，以促进一系列决策，如日志(multi-Paxos)。Paxos确保安全性和活性，并且支持集群成员的更改。其正确性已被证明，在通常情况下是有效的。</description>
    </item>
    
    <item>
      <title>MapReduce</title>
      <link>https://engineers.cool/posts/distribute-system/papers-read/mapreduce/</link>
      <pubDate>Wed, 02 Feb 2022 09:50:55 +0800</pubDate>
      
      <guid>https://engineers.cool/posts/distribute-system/papers-read/mapreduce/</guid>
      <description>Simplified Data Processing on Large Clusters.
 1. 介绍 原论文介绍就不翻译了，本文是从这里阅读原文时，为了更好的记忆消化，而做的个人翻译。
2. 编程模型 计算过程以一组key/values对为输入，然后产出一组key/values对输出。MapReduce库的用户使用两个函数表达整个计算过程：Map和Reduce。
 Map以一个key/values对为输入，然后生成一组key/values对中间数据。MapReduce库会统一组织所有Map生成的中间数据，把相同key（say as I）的所有values，传递给Reduce函数。 Reduce以一个key（say as I）以及相关的一组中间数据为入参。把这组数据合并为更小的集合。通常每次Reduce调用只返回0或1个值。中间数据通过一个迭代器传递给Reduce。这使得我们可以处理放不下内存的数据量。  2.1 举例 以统计超大文档集合中各个单词数量的问题为例。用户编写的代码如下（伪码）：
map(String key, String value): // key: 文档名 // value: 文档内容 for each word w in value: EmitIntermedite(w, &amp;quot;1&amp;quot;); reduce(String key, Iterator values): // key: 一个单词 // values：一系列的counts int result = 0; for each v in values: result += ParseInt(v); Emit(AsString(result)); map函数对每一个单词emit一个出现次数的关联数据（该例子中就是1）。reduce函数把这个单词所有emit的数据，加和在一起。
2.2 参数类型 虽然伪代码例子中的入参出参的类型是String，但是从逻辑上看，两个函数的参数类型具有关联性：
map(k1,v1) --------→ list(k2,v2) reduce(k2,list(v2)) --→ list(v2) 也就是说，入参的类型跟出参不是同一领域，中间数据跟出参是同一领域。</description>
    </item>
    
    <item>
      <title>The Google File System</title>
      <link>https://engineers.cool/posts/distribute-system/papers-read/gfs/</link>
      <pubDate>Wed, 02 Feb 2022 09:50:48 +0800</pubDate>
      
      <guid>https://engineers.cool/posts/distribute-system/papers-read/gfs/</guid>
      <description>摘要  原论文在这里
 我们设计并实现了谷歌文件系统（the Google File System, GFS）,一个为大型分布式数据密集型应用设计的可扩展的分布式的文件系统。他运行在不昂贵的商用硬件上，有容错能力，能为大量client提供整体地高性能服务。
与之前设计的分布式文件系统拥有许多相同目标的同时，我们的设计由 应用的负载 和 技术环境的现状与可预见的未来 驱动，这也导致与早先的文件系统的基本假设由显著背离。这使得我们复查历史决策并探讨不同的基本设计要点。
GFS很成功的满足了我们的存储需求。他作为存储平台在Google内部广泛部署，供需要处理或生成大数据集的在线服务、研发工作使用。当前最大的集群，由数千台机器&amp;amp;&amp;amp;磁盘组成，提供数百TB的存储能力，并提供数百的并发访问能力。
在这篇论文中，我们呈现支持分布式应用的分布式文件系统的接口设计，讨论设计的方方面面，并从微观基准和实际使用中报告测量结果。
1. 介绍 我们设计并实现了谷歌文件系统（the Google File System, GFS）,来满足Google数据处理快速增长的需求。与之前设计的分布式文件系统拥有许多相同目标，例如 性能、可扩展性、可靠性、可用性。然而，它的设计由 应用的负载 和 技术环境的现状与可预见的未来 驱动，这也导致与早先的文件系统的基本假设由显著背离。这使得我们复查历史决策并探讨不同的基本设计要点。
第一，系统构成组件的故障被认定是正常情况 而不是异常情况。这个文件系统由数百甚至数千台不昂贵的商品存储设备组成，并被数量可观的客户端访问。组件的数量和质量基本上确认在有限的时间内一些组件一定无法工作，而且一些组件无法从故障中恢复。我已经遇到过的问题有：应用程序bug、操作系统bug、人员操作错误、磁盘故障、内存故障、链接故障、网络故障、供电故障等。因此，持续监控、错误侦测、容错、自动恢复都必须是系统支持的能力。
第二，传统标准下文件是巨大的。几GB大小的文件很常见。每个文件包含许多应用对象，例如web文档。常常，我们处理由数十亿对象组成的快速增长的数据集时，即使文件系统本可以支持，管理数十亿大约KB大小的文件是很笨重的。因此，必须重新考虑 设计假设和参数，例如 I/O 操作和块大小。
第三，大部分文件修改是追加内容，很少是修改原内容。文件内的随机写更是基本没有。一旦写入，文件仅仅用来读，并且常常是顺序读。许多不同的数据都有这种特性。一些可能构成大型仓库供数据分析程序扫描。一些也许是在线程序持续生成的数据流。一些也许是归档数据。一些也许是一台数据生成要立即或以后被另外机器处理的临时数据。在这种大型文件访问模型下，性能优化的重点是追加写和原子保障，而在客户端缓存数据块完全没用。
第四，协同设计应用程序和文件系统API可以提高我们的灵活性，从而使整个系统受益。例如，放宽GFS的一致性模型来极大的简化文件系统，而不会给应用程序带来沉重的负担。我们引入了原子追加写操作，让多个客户端无需互相通信就可以并发的向一个文件追加内容。详细内容会在论文后续中讨论。
为了不同的目的，已经部署了多套GFS集群。最大的有超过1000个存储结点，超过300TB磁盘容量，并连续被不同机器的数百个client频繁访问。
2. 设计概览 2.1 基本假设 在位我们需求设计文件系统时，我们一直被既有挑战又有机遇的假设所引导。我们前面提到了一些关键的观察结果，现在更详细的列出我们的列假设。
 系统有总会故障的许多廉价的产品零部件组成。他必须持续不断的监控自己，检测、容忍并从组件故障中快速恢复。 系统存储适量的大文件。我们预估预估会有几百万个文件，每个文件通常100MB或者更大。几GB大小的文件是通常情况，需要被有效管理。小文件必须支持，但我们不需要针对他们做优化。 工作负荷主要由两种读组成：大文件顺序读，小文件随机读。在大文件顺序读中，每次操作通常读取几百KB或1MB或更多。来自同一客户端的连续操作通常会读取文件的连续区域。小文件随机读，通常在文件的任意位置读几KB。注重性能的应用程序通常对他们的小文件度进行批处理和排序，以便在文件中稳步前进，而不是来回读取。 工作负荷也会有许多大文件追加数据的顺序写。通常来说，操作大小跟读取差不多。一旦写好，数据基本不再修改。小文件的随机写也支持，但是不需要很高效。 系统必须为多客户端同时写同一文件做良好的设计与有效的实现。我们的文件常常用做 生产-消费 队列，或多路合并。多台机器的几百个客户端，会并发的向一个文件写。具有最小同步开销的原子性，必不可少。文件也许稍后读取，消费者也可以立即读取文件内容。 持续高带宽比低延迟更重要。我们的大多数目标应用程序都重视以高速率处理大量数据，而很少有对单个读取或写入的响应时间做严格要求。  2.2 接口 GFS提供了一套熟悉的系统接口，虽然并没有实现类似POSIX的标准API。文件又目录层级组织，并由路径名标识。我们支持了create、delete、open、close、read和write文件的一般接口。
此外，GFS有快照（snapshot）和记录追加（record append）操作。快照可以低成本的给文件或目录创建副本。记录追加可以在保证原子性的同时，支持多个独立客户端并发向同一个文件追加内容。这在实现多路合并 和 支持无需额外锁的多客户端快速追加内容的生产-消费队列。这种类型的文件，在建设分布式系统中非常有用。快照和记录追加会在3.4节和3.3节分别讨论。
2.3 架构 如图1所示，一个GFS集群，由一个master和多个数据服务器（chunkserver）组成，并被多个客户端访问。每一台设备都是商品Linux机器，运行着用户级服务器进程。只要机器资源允许，在同一台机器上运行chunkserver和客户端是OK的，并且由于运行不稳定的应用程序代码导致的地可靠性是可接受的。
文件被分为固定大小的chunk。每个chunk在被创建时都会被master分配一个全局的不可变的64bit的chunk handle。chunkserver在本地磁盘以Linux文件的形式存储chunk，并读取由 chunk handle和字节范围 标识 的chunk数据。为了可靠性，每个chunk在多台chunkserver做了副本。默认情况下，我们存储3份，用户也可以在不通的文件名字空间的区域指定不同级别的副本。
master维护所有文件系统的元数据。包括 名字空间、访问控制信息、文件到chunk的映射、chunk当前的位置等。它也管理系统纬度的活动，例如chunk租约管理、孤儿chunk的垃圾回收、chunkserver间的chunk迁移。master周期性的以HeartBeat信息的形式跟每个chunkserver通信，分发指令以及收集状态。</description>
    </item>
    
  </channel>
</rss>
