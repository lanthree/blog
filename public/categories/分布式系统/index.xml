<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>分布式系统 on Engineers Cool</title>
    <link>https://engineers.cool/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/</link>
    <description>Recent content in 分布式系统 on Engineers Cool</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 02 Feb 2022 09:51:01 +0800</lastBuildDate><atom:link href="https://engineers.cool/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Raft</title>
      <link>https://engineers.cool/posts/distribute-system/papers-read/raft/</link>
      <pubDate>Wed, 02 Feb 2022 09:51:01 +0800</pubDate>
      
      <guid>https://engineers.cool/posts/distribute-system/papers-read/raft/</guid>
      <description>In Search of an Understandable Consensus Algorithm
 摘要  原论文在这里
 Raft是一种用于管理副本日志的共识算法（Consensus Algorithm）。它提供一个等同于(multi-)Paxos的结果，效率跟Paxos一样，但是组织结构不同于Paxos；这令Raft比Paxos更利于理解，对于建设实际系统提供了更好的基础。为了增加可理解性，Raft分离了共识的关键元素，例如，leader选举、日志副本和安全(safety)，他主张更强的一致性，来减少必须要考虑的状态的数量。一项用户研究的结果表明，对于学生来说，Raft比Paxos更容易学习。Raft还包括一个改变集群中成员关系的新机制，它使用重叠的多数来保证安全。
1. 介绍 共识算法允许一批机器像 能够经受其部分成员故障的一致的群体 一样工作。因此，在建设可靠的大型软件系统中，共识算法承担着关键角色。在过去十年中，Paxos算法统治着共识算法的讨论：大部分共识的实现都是基于Paxos或受其影响，Paxos也变成教导学生共识算法的重要工具。
不幸的是，尽管人们极力尝试让它平易近人，Paxos还是非常难懂。此外，为了支持实际系统，它的架构需要复杂的变化。结果，系统的构建者和学生都在与Paxos作斗争。
在亲自跟Paxos斗争之后，我们开始寻找一个新的 能为系统构建和教育提供更好基础 共识算法。我们的方法不同寻常，因为我们的首要目标是可理解性：我们是否可以为实际系统定义一个共识算法，并用一种明显比Paxos容易学习的方式描述它？另外，我们希望算法能够促进直觉的发展，这对于系统构建者来说是必不可少的。重要的不仅是算法是否有效，还在于它能明显地被理解为什么有效。
这项工作的成果是一个称为Raft的共识算法。在设计Raft时，我们应用特殊的技术来提高可理解性，包括分解（Raft分解leader选举、日志副本和安全）和减少状态空间（相对于Paxos，Raft减少了非确定性程度和服务器之间保持一致性的方法）。一项涉及2所大学43名学生的用户研究表明，Raft明显比Paxos容易理解：在学习完两个算法后，其中33名学习对Raft问题的回答比Paxos问题要好。
Raft在许多方面与现有的共识算法相似（最明显的是，Oki和Liskov的Viewstamped Replication），但是特有几个新颖的特性：
 强leader（strong leader）：相对于其他共识算法，Raft使用一种更强形式的leader模式。例如，日志条目（log entries）只能从leader向其他server流动。这减缓了备份日志的管理，也让Raft更容易理解。 leader选举（leader election）：Raft使用随机的timer来选举leader。这仅仅在任何共识算法都需要的heartbeat上增加一点点机制，但是简单快速地解决了冲突问题。 成员关系变化（membership changes）：Raft用于更改集群中的服务器集的机制使用了一种新的联合共识（joint consensus）方法，在这种方法中，两种不同配置的大部分服务 在转换期间 重叠（overlap）。这允许集群在配置更改期间继续正常运行。  我们相信，无论出于教育目的 还是最为实现基础，Raft都比Paxos更优秀。Raft比其他算法都简单，可理解性更强；描述完整，足以满足实际系统的需要；它有几个开源实现，被多家公司使用；其安全特性已得到正式规定和证明； 其效率可与其他算法相媲美。
论文的其余部分介绍了副本状态机（replicated state machine）问题（第2节），讨论Paxos的优缺点（第 3节），描述我们实现可理解性的一般方法（第4节），介绍Raft共识算法（第5-8节），评估Raft（第9节），并讨论相关工作（第10节）。
2. 副本状态机 共识算法通常出现在副本状态机（replicated state machine）的上下文中。在这个方法中，一组server的状态机计算相同状态的相同副本，能在其中一些server故障时继续服务。副本状态机用于在分布式系统中解决许多容错问题。举个例子，拥有单集群master的大型系统 例如 GFS、HDFS、RAMCloud，通常用一个分离的副本状态机来管理leader选举和存储配置信息，达到leader crash还能继续服务的目的。副本状态机的例子包括Chubby、ZooKeeper。
如图1所示，副本状态机通常用副本日志实现。每个server存储包含一系列命令的日志，他们的状态机按日志顺序执行。每个日志包含同样顺序的同样命令，所以每个状态机按同样的命令处理同样的命令。因此，这些状态机是确定的，每个计算同样的状态，输出同样的序列。
保持副本日志的一致性就是共识算法的工作。server上的共识模块从多个client接收不同命令，并把命令添加到它的日志中。它与其他server上的共识模块交流，来确保，即使某些server故障，每个日志最终包含相同顺序的同样命令。一旦命令被合适的备份，每个server的状态机按日志顺序处理这些命令，输出被返回给那些client。最终结果，server看上去形成一个单一的、高可用的状态机。
实际系统的共识算法通常具有以下特性：
 算法确保在所有非拜占庭（non-Byzantine）条件下的安全性（safety，从不返回不正确的结果）,包括网络延迟、分区（partitions）、丢包、重入和重排序（reordering）等。  拜占庭问题。简单解释是：有间谍（叛徒），跟不同/相同对象返回的决策结果不一致。   只要任意大多数server可执行且能其他server/client通信，算法就是功能完好的（可用的）。因此，通常5台server的集群，可以容忍任意2台server故障。server假设的故障是停止工作；他们也许稍后会从稳定存储中恢复状态并重新加入集群。 算法不能依赖timing来确保日志的一致性：最坏情况下，时钟错误和极端的消息延迟能导致可用性问题。 通常情况下，只要集群的大多数成员响应了一轮RPC，命令就可以完成；少数速度较慢的服务器需要不影响整个系统的性能。  3. Paxos的问题是什么？ 在过去十年中，Leslit Lamport的Paxos协议几乎已经成为共识的同义词：它是在课堂中最常教授的协议，大多数共识的实现都将其作为起点。Paxos首先定义了一种 能够就单个决定达成一致 的协议，例如单个日志条目备份。我们称这个子集为single-decree Paxos。Paxos然后组合该协议的多个实例，以促进一系列决策，如日志(multi-Paxos)。Paxos确保安全性和活性，并且支持集群成员的更改。其正确性已被证明，在通常情况下是有效的。</description>
    </item>
    
    <item>
      <title>MapReduce</title>
      <link>https://engineers.cool/posts/distribute-system/papers-read/mapreduce/</link>
      <pubDate>Wed, 02 Feb 2022 09:50:55 +0800</pubDate>
      
      <guid>https://engineers.cool/posts/distribute-system/papers-read/mapreduce/</guid>
      <description>Simplified Data Processing on Large Clusters.
 1. 介绍 原论文介绍就不翻译了，本文是从这里阅读原文时，为了更好的记忆消化，而做的个人翻译。
2. 编程模型 计算过程以一组key/values对为输入，然后产出一组key/values对输出。MapReduce库的用户使用两个函数表达整个计算过程：Map和Reduce。
 Map以一个key/values对为输入，然后生成一组key/values对中间数据。MapReduce库会统一组织所有Map生成的中间数据，把相同key（say as I）的所有values，传递给Reduce函数。 Reduce以一个key（say as I）以及相关的一组中间数据为入参。把这组数据合并为更小的集合。通常每次Reduce调用只返回0或1个值。中间数据通过一个迭代器传递给Reduce。这使得我们可以处理放不下内存的数据量。  2.1 举例 以统计超大文档集合中各个单词数量的问题为例。用户编写的代码如下（伪码）：
map(String key, String value): // key: 文档名 // value: 文档内容 for each word w in value: EmitIntermedite(w, &amp;quot;1&amp;quot;); reduce(String key, Iterator values): // key: 一个单词 // values：一系列的counts int result = 0; for each v in values: result += ParseInt(v); Emit(AsString(result)); map函数对每一个单词emit一个出现次数的关联数据（该例子中就是1）。reduce函数把这个单词所有emit的数据，加和在一起。
2.2 参数类型 虽然伪代码例子中的入参出参的类型是String，但是从逻辑上看，两个函数的参数类型具有关联性：
map(k1,v1) --------→ list(k2,v2) reduce(k2,list(v2)) --→ list(v2) 也就是说，入参的类型跟出参不是同一领域，中间数据跟出参是同一领域。</description>
    </item>
    
    <item>
      <title>The Google File System</title>
      <link>https://engineers.cool/posts/distribute-system/papers-read/gfs/</link>
      <pubDate>Wed, 02 Feb 2022 09:50:48 +0800</pubDate>
      
      <guid>https://engineers.cool/posts/distribute-system/papers-read/gfs/</guid>
      <description>摘要  原论文在这里
 我们设计并实现了谷歌文件系统（the Google File System, GFS）,一个为大型分布式数据密集型应用设计的可扩展的分布式的文件系统。他运行在不昂贵的商用硬件上，有容错能力，能为大量client提供整体地高性能服务。
与之前设计的分布式文件系统拥有许多相同目标的同时，我们的设计由 应用的负载 和 技术环境的现状与可预见的未来 驱动，这也导致与早先的文件系统的基本假设由显著背离。这使得我们复查历史决策并探讨不同的基本设计要点。
GFS很成功的满足了我们的存储需求。他作为存储平台在Google内部广泛部署，供需要处理或生成大数据集的在线服务、研发工作使用。当前最大的集群，由数千台机器&amp;amp;&amp;amp;磁盘组成，提供数百TB的存储能力，并提供数百的并发访问能力。
在这篇论文中，我们呈现支持分布式应用的分布式文件系统的接口设计，讨论设计的方方面面，并从微观基准和实际使用中报告测量结果。
1. 介绍 我们设计并实现了谷歌文件系统（the Google File System, GFS）,来满足Google数据处理快速增长的需求。与之前设计的分布式文件系统拥有许多相同目标，例如 性能、可扩展性、可靠性、可用性。然而，它的设计由 应用的负载 和 技术环境的现状与可预见的未来 驱动，这也导致与早先的文件系统的基本假设由显著背离。这使得我们复查历史决策并探讨不同的基本设计要点。
第一，系统构成组件的故障被认定是正常情况 而不是异常情况。这个文件系统由数百甚至数千台不昂贵的商品存储设备组成，并被数量可观的客户端访问。组件的数量和质量基本上确认在有限的时间内一些组件一定无法工作，而且一些组件无法从故障中恢复。我已经遇到过的问题有：应用程序bug、操作系统bug、人员操作错误、磁盘故障、内存故障、链接故障、网络故障、供电故障等。因此，持续监控、错误侦测、容错、自动恢复都必须是系统支持的能力。
第二，传统标准下文件是巨大的。几GB大小的文件很常见。每个文件包含许多应用对象，例如web文档。常常，我们处理由数十亿对象组成的快速增长的数据集时，即使文件系统本可以支持，管理数十亿大约KB大小的文件是很笨重的。因此，必须重新考虑 设计假设和参数，例如 I/O 操作和块大小。
第三，大部分文件修改是追加内容，很少是修改原内容。文件内的随机写更是基本没有。一旦写入，文件仅仅用来读，并且常常是顺序读。许多不同的数据都有这种特性。一些可能构成大型仓库供数据分析程序扫描。一些也许是在线程序持续生成的数据流。一些也许是归档数据。一些也许是一台数据生成要立即或以后被另外机器处理的临时数据。在这种大型文件访问模型下，性能优化的重点是追加写和原子保障，而在客户端缓存数据块完全没用。
第四，协同设计应用程序和文件系统API可以提高我们的灵活性，从而使整个系统受益。例如，放宽GFS的一致性模型来极大的简化文件系统，而不会给应用程序带来沉重的负担。我们引入了原子追加写操作，让多个客户端无需互相通信就可以并发的向一个文件追加内容。详细内容会在论文后续中讨论。
为了不同的目的，已经部署了多套GFS集群。最大的有超过1000个存储结点，超过300TB磁盘容量，并连续被不同机器的数百个client频繁访问。
2. 设计概览 2.1 基本假设 在位我们需求设计文件系统时，我们一直被既有挑战又有机遇的假设所引导。我们前面提到了一些关键的观察结果，现在更详细的列出我们的列假设。
 系统有总会故障的许多廉价的产品零部件组成。他必须持续不断的监控自己，检测、容忍并从组件故障中快速恢复。 系统存储适量的大文件。我们预估预估会有几百万个文件，每个文件通常100MB或者更大。几GB大小的文件是通常情况，需要被有效管理。小文件必须支持，但我们不需要针对他们做优化。 工作负荷主要由两种读组成：大文件顺序读，小文件随机读。在大文件顺序读中，每次操作通常读取几百KB或1MB或更多。来自同一客户端的连续操作通常会读取文件的连续区域。小文件随机读，通常在文件的任意位置读几KB。注重性能的应用程序通常对他们的小文件度进行批处理和排序，以便在文件中稳步前进，而不是来回读取。 工作负荷也会有许多大文件追加数据的顺序写。通常来说，操作大小跟读取差不多。一旦写好，数据基本不再修改。小文件的随机写也支持，但是不需要很高效。 系统必须为多客户端同时写同一文件做良好的设计与有效的实现。我们的文件常常用做 生产-消费 队列，或多路合并。多台机器的几百个客户端，会并发的向一个文件写。具有最小同步开销的原子性，必不可少。文件也许稍后读取，消费者也可以立即读取文件内容。 持续高带宽比低延迟更重要。我们的大多数目标应用程序都重视以高速率处理大量数据，而很少有对单个读取或写入的响应时间做严格要求。  2.2 接口 GFS提供了一套熟悉的系统接口，虽然并没有实现类似POSIX的标准API。文件又目录层级组织，并由路径名标识。我们支持了create、delete、open、close、read和write文件的一般接口。
此外，GFS有快照（snapshot）和记录追加（record append）操作。快照可以低成本的给文件或目录创建副本。记录追加可以在保证原子性的同时，支持多个独立客户端并发向同一个文件追加内容。这在实现多路合并 和 支持无需额外锁的多客户端快速追加内容的生产-消费队列。这种类型的文件，在建设分布式系统中非常有用。快照和记录追加会在3.4节和3.3节分别讨论。
2.3 架构 如图1所示，一个GFS集群，由一个master和多个数据服务器（chunkserver）组成，并被多个客户端访问。每一台设备都是商品Linux机器，运行着用户级服务器进程。只要机器资源允许，在同一台机器上运行chunkserver和客户端是OK的，并且由于运行不稳定的应用程序代码导致的地可靠性是可接受的。
文件被分为固定大小的chunk。每个chunk在被创建时都会被master分配一个全局的不可变的64bit的chunk handle。chunkserver在本地磁盘以Linux文件的形式存储chunk，并读取由 chunk handle和字节范围 标识 的chunk数据。为了可靠性，每个chunk在多台chunkserver做了副本。默认情况下，我们存储3份，用户也可以在不通的文件名字空间的区域指定不同级别的副本。
master维护所有文件系统的元数据。包括 名字空间、访问控制信息、文件到chunk的映射、chunk当前的位置等。它也管理系统纬度的活动，例如chunk租约管理、孤儿chunk的垃圾回收、chunkserver间的chunk迁移。master周期性的以HeartBeat信息的形式跟每个chunkserver通信，分发指令以及收集状态。</description>
    </item>
    
    <item>
      <title>CAP</title>
      <link>https://engineers.cool/posts/distribute-system/cap/</link>
      <pubDate>Wed, 02 Feb 2022 09:36:38 +0800</pubDate>
      
      <guid>https://engineers.cool/posts/distribute-system/cap/</guid>
      <description>CAP理论1 在理论计算机科学中，CAP定理（CAP theorem），又被称作布鲁尔定理（Brewer&amp;rsquo;s theorem），它指出对于一个分布式计算系统来说，不可能同时满足以下三点：
 一致性（Consistency） （等同于所有节点访问同一份最新的数据副本） 可用性（Availability）（每次请求都能获取到非错的响应——但是不保证获取的数据为最新数据） 分区容错性（Partition tolerance）（以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择。）  根据定理，分布式系统只能满足三项中的两项而不可能满足全部三项[4]。理解CAP理论的最简单方式是想象两个节点分处分区两侧。允许至少一个节点更新状态会导致数据不一致，即丧失了C性质。如果为了保证数据一致性，将分区一侧的节点设置为不可用，那么又丧失了A性质。除非两个节点可以互相通信，才能既保证C又保证A，这又会导致丧失P性质。
数据一致性2 分布式系统中，一般为了容错性，会引入多副本，多个副本间需要同步状态，就引入了一致性问题。一致性问题包含数据一致性和事务一致性（指ACID）。
DDIA第五章更详细的给出了集中供暖需要备份数据的情况：
 为了保持需要地理上离用户更近（降低延迟） 为了允许系统在某些部分故障时仍然能持续工作（增加可用性） 为了扩充读服务的机器数量（增加读性能）  其实只有两类数据一致性，强一致性与弱一致性。强一致性也叫做线性一致性，除此以外，所有其他的一致性都是弱一致性的特殊情况。所谓强一致性，即复制是同步的，弱一致性，即复制是异步的。
强一致性 强一致性可以保证从库有与主库一致的数据。如果主库突然宕机，我们仍可以保证数据完整。但如果从库宕机或网络阻塞，主库就无法完成写入操作。
在实践中，我们通常使一个从库是同步的，而其他的则是异步的。如果这个同步的从库出现问题，则使另一个异步从库同步。这可以确保永远有两个节点拥有完整数据：主库和同步从库。 这种配置称为半同步。
弱一致性 最终一致 容忍节点故障只是需要复制的一个原因。另两个原因是可扩展性和降低延迟。
单领导者的主从复制算法要求所有写入都由单个节点处理，但只读查询可以由任何节点处理。对于读多写少的场景，我们往往创建很多从库，并将读请求分散到所有的从库上去。这样能减小主库的负载，并允许向最近的节点发送读请求。当然这只适用于异步复制——如果尝试同步复制，则单个节点故障将使整个系统无法写入。
当用户从异步从库读取时，如果此异步从库落后，他可能会看到过时的信息。这种不一致只是一个暂时的状态——如果等待一段时间，从库最终会赶上并与主库保持一致。这称为最终一致性。
最终两个字用得很微妙，因为从写入主库到反映至从库之间的延迟，可能仅仅是几分之一秒，也可能是几个小时。
读写一致（读己之写一致） 手机刷虎扑的时候经常遇到，回复某人的帖子然后想马上查看，但我刚提交的回复可能尚未到达从库，看起来好像是刚提交的数据丢失了，很不爽。
在这种情况下，我们需要读写一致性，也称为读己之写一致性。它可以保证，如果用户刷新页面，他们总会看到自己刚提交的任何更新。它不会对其他用户的写入做出承诺，其他用户的更新可能稍等才会看到，但它保证用户自己提交的数据能马上被自己看到。
如何实现读写一致性？
最简单的方案，对于某些特定的内容，都从主库读。举个例子，知乎个人主页信息只能由用户本人编辑，而不能由其他人编辑。因此，永远从主库读取用户自己的个人主页，从从库读取其他用户的个人主页。
如果应用中的大部分内容都可能被用户编辑，那这种方法就没用了。在这种情况下可以使用其他标准来决定是否从主库读取，例如可以记录每个用户最后一次写入主库的时间，一分钟内都从主库读，同时监控从库的最后同步时间，任何超过一分钟没有更新的从库不响应查询。
还有一种更好的方法是，客户端可以在本地记住最近一次写入的时间戳，发起请求时带着此时间戳。从库提供任何查询服务前，需确保该时间戳前的变更都已经同步到了本从库中。如果当前从库不够新，则可以从另一个从库读，或者等待从库追赶上来。
单调读 用户从某从库查询到了一条记录，再次刷新后发现此记录不见了，就像遇到时光倒流。如果用户从不同从库进行多次读取，就可能发生这种情况。
单调读可以保证这种异常不会发生。单调读意味着如果一个用户进行多次读取时，绝对不会遇到时光倒流，即如果先前读取到较新的数据，后续读取不会得到更旧的数据。单调读比强一致性更弱，比最终一致性更强。
实现单调读取的一种方式是确保每个用户总是从同一个节点进行读取（不同的用户可以从不同的节点读取），比如可以基于用户ID的哈希值来选择节点，而不是随机选择节点。
因果一致性 在本文中阐述因果一致性可能并不是一个很好的时机，因为它往往发生在分区（也称为分片）的分布式数据库中。
分区后，每个节点并不包含全部数据。不同的节点独立运行，因此不存在全局写入顺序。如果用户A提交一个问题，用户B提交了回答。问题写入了节点A，回答写入了节点B。因为同步延迟，发起查询的用户可能会先看到回答，再看到问题。
为了防止这种异常，需要另一种类型的保证：因果一致性。 即如果一系列写入按某个逻辑顺序发生，那么任何人读取这些写入时，会看见它们以正确的逻辑顺序出现。
这是一个听起来简单，实际却很难解决的问题。一种方案是应用保证将问题和对应的回答写入相同的分区。但并不是所有的数据都能如此轻易地判断因果依赖关系。如果有兴趣可以搜索向量时钟深入此问题。
  CAP定理&amp;#160;&amp;#x21a9;&amp;#xfe0e;
 通俗易懂 强一致性、弱一致性、最终一致性、读写一致性、单调读、因果一致性 的区别与联系&amp;#160;&amp;#x21a9;&amp;#xfe0e;
   </description>
    </item>
    
  </channel>
</rss>
